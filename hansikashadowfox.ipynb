{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9c22111-db34-4424-bc37-9206a962bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sun was shining brightly in the clear blue sky, and a gentle breeze rustled the leaves of the tall trees. People were out enjoying the beautiful weather, some sitting in the park, others taking a leisurely stroll along the riverbank. Children were playing games, and laughter filled the air.\\n', '\\n', 'As the day turned into evening, the temperature started to drop, and the sky transformed into a canvas of vibrant colors. Families gathered for picnics, and the smell of barbecues wafted through the air. It was a perfect day for a picnic by the lake.\\n', '\\n', 'In the distance, you could hear the sound of live music coming from a local band, and people began to gather around the stage to enjoy the performance. The atmosphere was electric, and the music had everyone swaying to the beat.\\n']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'next_word_predictor.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "print(data[:5])  # Display the first 5 lines of the text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8aea22e5-8091-484e-b05d-b0b46362dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2693637e-8b7e-4bb1-9baa-5938e481d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Sentence tokenizer\n",
    "nltk.download('stopwords')  # Stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d343aeaf-faf0-45c2-9262-0c8fa377237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9b45a1c-02c8-4f67-816a-61c28072a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text from the file before processing\n",
    "file_path = 'next_word_predictor.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Now process the actual text\n",
    "tokens = preprocess_text(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ea6c404-b4d4-42cb-a8bd-d62e38e5c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example prefixes: [('sun', 'shining'), ('shining', 'brightly'), ('brightly', 'clear'), ('clear', 'blue'), ('blue', 'sky')]\n",
      "Example next-word choices: [['brightly'], ['clear'], ['blue'], ['sky'], ['gentle', 'kangaroos', 'framing']]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(tokens, n=3):\n",
    "    ngrams = defaultdict(list)\n",
    "\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        prefix, next_word = tuple(tokens[i:i+n-1]), tokens[i+n-1]\n",
    "        ngrams[prefix].append(next_word)\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "# Build a trigram model (n=3)\n",
    "n = 3\n",
    "ngram_model = build_ngram_model(tokens, n)\n",
    "\n",
    "# Print some example n-grams\n",
    "print(\"Example prefixes:\", list(ngram_model.keys())[:5])\n",
    "print(\"Example next-word choices:\", list(ngram_model.values())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c665e44-b8ef-4f0f-af10-d93d18d0f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word after '['sun', 'shining']': brightly\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def predict_next_word(ngram_model, prev_words):\n",
    "    prev_words = tuple(prev_words[-2:])  # Use the last 2 words for prediction (trigram)\n",
    "    \n",
    "    if prev_words in ngram_model:\n",
    "        return random.choice(ngram_model[prev_words])  # Choose a random next word\n",
    "    else:\n",
    "        return None  # No prediction found\n",
    "\n",
    "# Example prediction\n",
    "prev_words = [\"sun\", \"shining\"]\n",
    "predicted_word = predict_next_word(ngram_model, prev_words)\n",
    "print(f\"Predicted next word after '{prev_words}': {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4081ab1c-3ed5-423e-b58a-08522d4f7661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['sun', 'shining'] -> Predicted Next Word: brightly\n",
      "Input: ['brightly', 'clear'] -> Predicted Next Word: blue\n",
      "Input: ['blue', 'sky'] -> Predicted Next Word: framing\n",
      "Input: ['gentle', 'kangaroos'] -> Predicted Next Word: None\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    [\"sun\", \"shining\"],\n",
    "    [\"brightly\", \"clear\"],\n",
    "    [\"blue\", \"sky\"],\n",
    "    [\"gentle\", \"kangaroos\"]\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predicted = predict_next_word(ngram_model, sentence)\n",
    "    print(f\"Input: {sentence} -> Predicted Next Word: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "719963fd-7532-4159-b442-4427c86d910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example trigram probabilities: [(('sun', 'shining'), {'brightly': 0.0004130524576621231}), (('shining', 'brightly'), {'clear': 0.0004130524576621231}), (('brightly', 'clear'), {'blue': 0.0004130524576621231}), (('clear', 'blue'), {'sky': 0.0004130524576621231}), (('blue', 'sky'), {'gentle': 0.00041288191577208916, 'kangaroos': 0.00041288191577208916, 'framing': 0.00041288191577208916})]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model_smooth(tokens, n=3):\n",
    "    ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        prefix, next_word = tuple(tokens[i:i+n-1]), tokens[i+n-1]\n",
    "        ngram_counts[prefix][next_word] += 1  # Count occurrences\n",
    "\n",
    "    # Convert counts to probabilities with Laplace Smoothing\n",
    "    vocab_size = len(set(tokens))  # Unique words in vocabulary\n",
    "    for prefix, next_word_counts in ngram_counts.items():\n",
    "        total_count = sum(next_word_counts.values())\n",
    "        ngram_counts[prefix] = {word: (count + 1) / (total_count + vocab_size) for word, count in next_word_counts.items()}  # Add-1 smoothing\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "# Build the smoothed trigram model\n",
    "ngram_model_smooth = build_ngram_model_smooth(tokens, n=3)\n",
    "\n",
    "# Print an example\n",
    "print(\"Example trigram probabilities:\", list(ngram_model_smooth.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0f739db-aa4d-4e9a-94d0-bff5de70692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\saniy\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "   ---------------------------------------- 0.0/376.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/376.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/376.0 MB 7.3 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 3.4/376.0 MB 7.2 MB/s eta 0:00:52\n",
      "    --------------------------------------- 5.0/376.0 MB 7.2 MB/s eta 0:00:52\n",
      "    --------------------------------------- 6.3/376.0 MB 6.9 MB/s eta 0:00:54\n",
      "    --------------------------------------- 7.6/376.0 MB 6.8 MB/s eta 0:00:55\n",
      "    --------------------------------------- 8.4/376.0 MB 6.3 MB/s eta 0:00:58\n",
      "    --------------------------------------- 9.2/376.0 MB 6.0 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 10.2/376.0 MB 5.7 MB/s eta 0:01:04\n",
      "   - -------------------------------------- 10.7/376.0 MB 5.5 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 11.3/376.0 MB 5.1 MB/s eta 0:01:12\n",
      "   - -------------------------------------- 11.8/376.0 MB 5.0 MB/s eta 0:01:14\n",
      "   - -------------------------------------- 13.1/376.0 MB 5.1 MB/s eta 0:01:12\n",
      "   - -------------------------------------- 14.9/376.0 MB 5.2 MB/s eta 0:01:09\n",
      "   - -------------------------------------- 15.5/376.0 MB 5.1 MB/s eta 0:01:11\n",
      "   - -------------------------------------- 16.8/376.0 MB 5.1 MB/s eta 0:01:11\n",
      "   - -------------------------------------- 18.4/376.0 MB 5.3 MB/s eta 0:01:08\n",
      "   -- ------------------------------------- 19.9/376.0 MB 5.4 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 21.0/376.0 MB 5.4 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 22.5/376.0 MB 5.5 MB/s eta 0:01:05\n",
      "   -- ------------------------------------- 23.9/376.0 MB 5.6 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 26.0/376.0 MB 5.7 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 28.3/376.0 MB 6.0 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 30.7/376.0 MB 6.2 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 33.0/376.0 MB 6.4 MB/s eta 0:00:54\n",
      "   --- ------------------------------------ 34.9/376.0 MB 6.5 MB/s eta 0:00:53\n",
      "   --- ------------------------------------ 37.0/376.0 MB 6.6 MB/s eta 0:00:52\n",
      "   ---- ----------------------------------- 39.1/376.0 MB 6.8 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 41.4/376.0 MB 6.9 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 43.8/376.0 MB 7.1 MB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 46.4/376.0 MB 7.2 MB/s eta 0:00:46\n",
      "   ----- ---------------------------------- 48.8/376.0 MB 7.4 MB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 51.1/376.0 MB 7.5 MB/s eta 0:00:44\n",
      "   ----- ---------------------------------- 53.0/376.0 MB 7.6 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 55.3/376.0 MB 7.7 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 56.6/376.0 MB 7.6 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 57.4/376.0 MB 7.5 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 57.9/376.0 MB 7.4 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 58.7/376.0 MB 7.3 MB/s eta 0:00:44\n",
      "   ------ --------------------------------- 60.0/376.0 MB 7.3 MB/s eta 0:00:44\n",
      "   ------ --------------------------------- 61.9/376.0 MB 7.3 MB/s eta 0:00:44\n",
      "   ------ --------------------------------- 64.0/376.0 MB 7.3 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 66.1/376.0 MB 7.4 MB/s eta 0:00:42\n",
      "   ------- -------------------------------- 66.6/376.0 MB 7.3 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 67.1/376.0 MB 7.2 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 67.9/376.0 MB 7.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 68.9/376.0 MB 7.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 70.5/376.0 MB 7.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 71.6/376.0 MB 7.0 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 72.9/376.0 MB 7.0 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 74.7/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 75.8/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 77.1/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 78.4/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 80.0/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 81.3/376.0 MB 7.0 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 82.1/376.0 MB 6.9 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 82.8/376.0 MB 6.9 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 83.6/376.0 MB 6.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 85.2/376.0 MB 6.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 87.0/376.0 MB 6.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 89.1/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 91.2/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 93.3/376.0 MB 7.0 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 95.7/376.0 MB 7.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 98.0/376.0 MB 7.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 99.9/376.0 MB 7.2 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 101.2/376.0 MB 7.1 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 102.0/376.0 MB 7.1 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 103.3/376.0 MB 7.1 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 103.8/376.0 MB 7.0 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 104.6/376.0 MB 7.0 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 105.1/376.0 MB 6.9 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 105.9/376.0 MB 6.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 106.7/376.0 MB 6.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 107.2/376.0 MB 6.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 108.0/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 108.5/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 109.8/376.0 MB 6.6 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 111.4/376.0 MB 6.6 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 112.7/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 114.3/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 116.4/376.0 MB 6.7 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 118.0/376.0 MB 6.7 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 119.8/376.0 MB 6.7 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 121.9/376.0 MB 6.8 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 124.3/376.0 MB 6.8 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 126.6/376.0 MB 6.9 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 129.0/376.0 MB 6.9 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 131.1/376.0 MB 6.9 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 132.6/376.0 MB 7.0 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 135.0/376.0 MB 7.0 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 137.1/376.0 MB 7.0 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 138.7/376.0 MB 7.0 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 140.0/376.0 MB 7.0 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 142.1/376.0 MB 7.1 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 144.2/376.0 MB 7.1 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 146.3/376.0 MB 7.1 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 148.6/376.0 MB 7.2 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 150.5/376.0 MB 7.2 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 151.8/376.0 MB 7.2 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 153.9/376.0 MB 7.2 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 156.0/376.0 MB 7.2 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 158.1/376.0 MB 7.2 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 159.4/376.0 MB 7.3 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 160.2/376.0 MB 7.2 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 160.7/376.0 MB 7.2 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 161.7/376.0 MB 7.1 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 162.8/376.0 MB 7.1 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 163.3/376.0 MB 7.1 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 163.8/376.0 MB 7.0 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 164.9/376.0 MB 7.0 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 165.9/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 166.7/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 168.6/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 170.1/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 171.4/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 172.5/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 173.3/376.0 MB 7.0 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 174.3/376.0 MB 6.9 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 175.9/376.0 MB 6.9 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 178.3/376.0 MB 7.0 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 180.1/376.0 MB 7.0 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 182.5/376.0 MB 7.0 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 185.1/376.0 MB 7.0 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 187.4/376.0 MB 7.1 MB/s eta 0:00:27\n",
      "   -------------------- ------------------- 190.1/376.0 MB 7.1 MB/s eta 0:00:27\n",
      "   -------------------- ------------------- 192.4/376.0 MB 7.2 MB/s eta 0:00:26\n",
      "   -------------------- ------------------- 194.8/376.0 MB 7.2 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 197.4/376.0 MB 7.2 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 199.8/376.0 MB 7.3 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 202.1/376.0 MB 7.3 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 204.7/376.0 MB 7.3 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 207.1/376.0 MB 7.4 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 209.7/376.0 MB 7.4 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 212.1/376.0 MB 7.4 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 214.4/376.0 MB 7.4 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 217.3/376.0 MB 7.5 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 219.7/376.0 MB 7.5 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 222.0/376.0 MB 7.5 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 224.4/376.0 MB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 227.0/376.0 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 229.4/376.0 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 232.0/376.0 MB 7.7 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 234.4/376.0 MB 7.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 236.7/376.0 MB 7.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 239.1/376.0 MB 7.8 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 241.4/376.0 MB 7.8 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 244.1/376.0 MB 7.9 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 246.4/376.0 MB 7.9 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 248.8/376.0 MB 8.0 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 251.4/376.0 MB 8.0 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 253.8/376.0 MB 8.1 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 256.1/376.0 MB 8.1 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 258.7/376.0 MB 8.2 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 261.1/376.0 MB 8.2 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 262.7/376.0 MB 8.2 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 264.5/376.0 MB 8.3 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 266.9/376.0 MB 8.3 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 269.2/376.0 MB 8.3 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 271.6/376.0 MB 8.4 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 273.9/376.0 MB 8.4 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 276.3/376.0 MB 8.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 278.9/376.0 MB 8.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 281.3/376.0 MB 8.4 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 283.6/376.0 MB 8.5 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 286.3/376.0 MB 8.5 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 288.6/376.0 MB 8.5 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 291.2/376.0 MB 8.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 293.3/376.0 MB 8.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 296.0/376.0 MB 8.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 297.8/376.0 MB 8.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 299.4/376.0 MB 8.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 301.7/376.0 MB 8.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 304.1/376.0 MB 8.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 306.4/376.0 MB 8.5 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 308.8/376.0 MB 8.5 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 310.6/376.0 MB 8.5 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 312.5/376.0 MB 8.5 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 314.3/376.0 MB 8.6 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 316.7/376.0 MB 8.6 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 318.5/376.0 MB 8.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 320.9/376.0 MB 8.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 323.0/376.0 MB 8.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 325.3/376.0 MB 8.6 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 327.7/376.0 MB 8.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 330.0/376.0 MB 8.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 332.4/376.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 334.8/376.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 337.1/376.0 MB 9.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 339.5/376.0 MB 9.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 340.8/376.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 342.1/376.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 343.1/376.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 343.9/376.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 346.0/376.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 348.1/376.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 350.2/376.0 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 352.1/376.0 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 353.9/376.0 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 355.7/376.0 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 357.6/376.0 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 359.7/376.0 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 361.8/376.0 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 363.6/376.0 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 365.4/376.0 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  367.5/376.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  369.4/376.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  371.2/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  373.3/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.7/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 376.0/376.0 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/26.4 MB 11.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/26.4 MB 11.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.6/26.4 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/26.4 MB 10.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/26.4 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.1/26.4 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 15.5/26.4 MB 10.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.6/26.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.4/26.4 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.2/26.4 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.6/26.4 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.5 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-win_amd64.whl (306 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.0 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce58ad3-24c2-4dbd-952e-74814fe5e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=2),  # Embedding layer\n",
    "    LSTM(100, return_sequences=False),  # LSTM layer\n",
    "    Dense(vocab_size, activation=\"softmax\")  # Output layer (probabilities)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f475f2c-aa40-470b-95f6-2bd3be5cc52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (16284, 2), Shape of y: (16284, 4842)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([\" \".join(tokens)])  # Fit tokenizer on all words\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Total words (+1 for padding)\n",
    "\n",
    "# Convert words to sequences\n",
    "sequences = []\n",
    "for i in range(2, len(tokens)):  # Use trigrams (2 previous words -> next word)\n",
    "    seq = tokenizer.texts_to_sequences([\" \".join(tokens[i-2:i+1])])[0]\n",
    "    sequences.append(seq)\n",
    "\n",
    "# Convert to numpy array\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]  # Last word is the target\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)  # One-hot encode output\n",
    "\n",
    "# Print shape\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d529edb-44f6-4bb5-8222-26847cfdc115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.0134 - loss: 8.2719\n",
      "Epoch 2/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.0174 - loss: 7.5145\n",
      "Epoch 3/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.0144 - loss: 7.3775\n",
      "Epoch 4/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.0159 - loss: 7.2398\n",
      "Epoch 5/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.0153 - loss: 7.0445\n",
      "Epoch 6/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.0197 - loss: 6.8642\n",
      "Epoch 7/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.0218 - loss: 6.7330\n",
      "Epoch 8/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.0289 - loss: 6.5971\n",
      "Epoch 9/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.0336 - loss: 6.4100\n",
      "Epoch 10/10\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.0398 - loss: 6.2767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20ebefdc710>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X, y, epochs=10, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b0256dc-7afd-4e9d-b9d5-10389771c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "Predicted next word after '['sun', 'shines']': germany\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_next_word_lstm(model, tokenizer, text):\n",
    "    seq = tokenizer.texts_to_sequences([\" \".join(text[-2:])])[0]  # Convert to sequence\n",
    "    seq = np.array(seq).reshape(1, -1)  # Reshape for model\n",
    "    pred = model.predict(seq)  # Predict next word\n",
    "    word_index = np.argmax(pred)  # Get highest probability word index\n",
    "    return tokenizer.index_word.get(word_index, None)  # Convert back to word\n",
    "\n",
    "# Example prediction\n",
    "input_text = [\"sun\", \"shines\"]\n",
    "predicted_word = predict_next_word_lstm(model, tokenizer, input_text)\n",
    "print(f\"Predicted next word after '{input_text}': {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacaabf-4229-4ea2-887c-62f6cab867f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_next_word_lstm(model, tokenizer, text):\n",
    "    seq = tokenizer.texts_to_sequences([\" \".join(text[-2:])])[0]  # Convert to sequence\n",
    "    seq = np.array(seq).reshape(1, -1)  # Reshape for model\n",
    "    pred = model.predict(seq)  # Predict next word\n",
    "    word_index = np.argmax(pred)  # Get highest probability word index\n",
    "    return tokenizer.index_word.get(word_index, None)  # Convert back to word\n",
    "\n",
    "# Example prediction\n",
    "input_text = [\"sun\", \"shines\"]\n",
    "predicted_word = predict_next_word_lstm(model, tokenizer, input_text)\n",
    "print(f\"Predicted next word after '{input_text}': {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b9ec2a7-82a0-42a2-aee0-59b33b1daada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Sequence: [126, 748]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Raw Predictions: [[1.2230682e-06 3.4477976e-06 3.1809361e-06 ... 9.5695659e-06\n",
      "  4.5260171e-05 1.1876161e-04]]\n",
      "Predicted Word Index: 379\n",
      "Predicted Word: germany\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"sun\", \"shines\"]\n",
    "seq = tokenizer.texts_to_sequences([\" \".join(input_text[-2:])])[0]  # Convert to sequence\n",
    "print(\"Converted Sequence:\", seq)\n",
    "\n",
    "seq = np.array(seq).reshape(1, -1)  # Reshape for model input\n",
    "pred = model.predict(seq)  # Get prediction probabilities\n",
    "\n",
    "print(\"Raw Predictions:\", pred)\n",
    "print(\"Predicted Word Index:\", np.argmax(pred))\n",
    "print(\"Predicted Word:\", tokenizer.index_word.get(np.argmax(pred), \"Unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e709f79e-016c-494c-8b01-c7b8a15ccad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Sentence tokenizer\n",
    "nltk.download('stopwords')  # Stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde01f8b-3943-41ba-803a-09797ee32536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3ee6a2-e548-4121-935b-21f81e42742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few tokens: ['nextwordpredictortxt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # ✅ Corrected List Comprehension\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    \n",
    "    return tokens  \n",
    "\n",
    "# Example dataset (Replace with your actual dataset)\n",
    "file_path = 'next_word_predictor.txt'\n",
    "\n",
    "# Apply preprocessing\n",
    "tokens = preprocess_text(file_path)\n",
    "\n",
    "# Print first 10 tokens to check\n",
    "print(\"First few tokens:\", tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07e52ea8-76d6-4129-88f3-d022297d3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'next_word_predictor.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Now process the actual text\n",
    "tokens = preprocess_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "030b0a2f-a8a1-4648-9f57-a034350bb94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (16284, 2), Shape of y: (16284, 4842)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([\" \".join(tokens)])  # Fit tokenizer on all words\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Total words (+1 for padding)\n",
    "\n",
    "# Convert words to sequences\n",
    "sequences = []\n",
    "for i in range(2, len(tokens)):  # Use trigrams (2 previous words -> next word)\n",
    "    seq = tokenizer.texts_to_sequences([\" \".join(tokens[i-2:i+1])])[0]\n",
    "    sequences.append(seq)\n",
    "\n",
    "# Convert to numpy array\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]  # Last word is the target\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)  # One-hot encode output\n",
    "\n",
    "# Print shape\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84b70e8d-8e21-4970-9c71-60e4ea1fd508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=2),  # Bigger embedding layer\n",
    "    LSTM(256, return_sequences=False),  # More LSTM units for better learning\n",
    "    Dense(vocab_size, activation=\"softmax\")  # Output layer (word prediction)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a755aca4-1d2d-4689-8ccc-daa3ef852f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.0126 - loss: 8.2283\n",
      "Epoch 2/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.0162 - loss: 7.4925\n",
      "Epoch 3/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.0156 - loss: 7.2278\n",
      "Epoch 4/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.0180 - loss: 6.9283\n",
      "Epoch 5/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.0271 - loss: 6.6676\n",
      "Epoch 6/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.0341 - loss: 6.4247\n",
      "Epoch 7/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.0505 - loss: 6.0411\n",
      "Epoch 8/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.0666 - loss: 5.6772\n",
      "Epoch 9/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.1055 - loss: 5.1935\n",
      "Epoch 10/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.1479 - loss: 4.7060\n",
      "Epoch 11/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.2310 - loss: 4.1910\n",
      "Epoch 12/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.3232 - loss: 3.6522\n",
      "Epoch 13/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.4324 - loss: 3.1007\n",
      "Epoch 14/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - accuracy: 0.5027 - loss: 2.6599\n",
      "Epoch 15/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.5790 - loss: 2.2521\n",
      "Epoch 16/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6147 - loss: 1.9694\n",
      "Epoch 17/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6581 - loss: 1.7174\n",
      "Epoch 18/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.6929 - loss: 1.5120\n",
      "Epoch 19/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.7221 - loss: 1.3472\n",
      "Epoch 20/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7433 - loss: 1.2054\n",
      "Epoch 21/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.7661 - loss: 1.0929\n",
      "Epoch 22/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.7859 - loss: 0.9845\n",
      "Epoch 23/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8054 - loss: 0.8848\n",
      "Epoch 24/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8132 - loss: 0.8195\n",
      "Epoch 25/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.8164 - loss: 0.7687\n",
      "Epoch 26/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.8300 - loss: 0.7069\n",
      "Epoch 27/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.8341 - loss: 0.6694\n",
      "Epoch 28/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8361 - loss: 0.6334\n",
      "Epoch 29/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.8441 - loss: 0.6026\n",
      "Epoch 30/30\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.8485 - loss: 0.5668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fc5746b800>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=30, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9101c2d-2ebf-4e67-9e2b-81bc2bb5318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Get the maximum sequence length used in training\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78b2e90c-b947-4e2b-bfe0-1c75e6aa8414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: brightly\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_next_word(input_text, model, tokenizer, max_sequence_len):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    input_seq = np.pad(input_seq, (max_sequence_len - len(input_seq), 0), mode='constant')\n",
    "    input_seq = np.array([input_seq])\n",
    "    \n",
    "    predicted_probs = model.predict(input_seq, verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Test Example:\n",
    "print(\"Prediction:\", predict_next_word(\"The sun was shining\", model, tokenizer, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ef1bb55-8bd6-4c2c-a30a-db987a17a884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: access\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_next_word(input_text, model, tokenizer, max_sequence_len):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    input_seq = np.pad(input_seq, (max_sequence_len - len(input_seq), 0), mode='constant')\n",
    "    input_seq = np.array([input_seq])\n",
    "    \n",
    "    predicted_probs = model.predict(input_seq, verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Test Example:\n",
    "print(\"Prediction:\", predict_next_word(\"and a drawbridge provided\", model, tokenizer, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcf607-5f22-4e76-a91d-fbc3fd02060f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
